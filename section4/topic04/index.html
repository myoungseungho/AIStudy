<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>활성화 함수는 왜 필요해? - Section 4</title>
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            font-family: 'Pretendard', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
            background: #fafafa;
            color: #333;
            line-height: 1.9;
        }
        .container { max-width: 800px; margin: 0 auto; padding: 60px 20px; }
        .breadcrumb { margin-bottom: 24px; font-size: 0.9rem; }
        .breadcrumb a { color: #666; text-decoration: none; }
        .breadcrumb a:hover { text-decoration: underline; }
        .breadcrumb span { color: #999; margin: 0 8px; }
        .header { margin-bottom: 48px; }
        .topic-number {
            display: inline-block;
            background: #9C27B0;
            color: white;
            padding: 6px 16px;
            border-radius: 20px;
            font-size: 0.85rem;
            font-weight: 600;
            margin-bottom: 16px;
        }
        .header h1 { font-size: 2rem; font-weight: 700; color: #111; margin-bottom: 12px; }
        .header p { color: #666; font-size: 1.1rem; }
        .conversation {
            background: white;
            border-radius: 16px;
            padding: 32px;
            margin: 32px 0;
            border: 1px solid #e0e0e0;
        }
        .conversation h2 {
            font-size: 1.3rem;
            color: #9C27B0;
            margin-bottom: 24px;
            padding-bottom: 12px;
            border-bottom: 2px solid #f3e5f5;
        }
        .chat { margin-bottom: 20px; }
        .q {
            background: #fff3e0;
            border-left: 4px solid #ff9800;
            padding: 16px 20px;
            margin: 16px 0;
            border-radius: 0 12px 12px 0;
            font-weight: 500;
        }
        .q::before { content: "Q. "; color: #e65100; font-weight: 700; }
        .a { padding: 8px 0 8px 20px; border-left: 4px solid #e0e0e0; margin: 12px 0; }
        .a p { margin-bottom: 12px; }
        .a p:last-child { margin-bottom: 0; }
        .highlight {
            background: #e3f2fd;
            padding: 2px 8px;
            border-radius: 4px;
            font-weight: 600;
            color: #1565c0;
        }
        .key-point {
            background: #f3e5f5;
            padding: 2px 8px;
            border-radius: 4px;
            font-weight: 600;
            color: #6a1b9a;
        }
        .warning {
            background: #ffcdd2;
            padding: 2px 8px;
            border-radius: 4px;
            font-weight: 600;
            color: #c62828;
        }
        .code-block {
            background: #263238;
            border-radius: 8px;
            padding: 16px 20px;
            margin: 16px 0;
            font-family: 'Consolas', monospace;
            font-size: 0.9rem;
            color: #eceff1;
            overflow-x: auto;
            line-height: 1.6;
            white-space: pre-wrap;
        }
        .compare-table {
            width: 100%;
            border-collapse: collapse;
            margin: 16px 0;
            font-size: 0.9rem;
        }
        .compare-table th, .compare-table td {
            padding: 12px;
            border: 1px solid #e0e0e0;
            text-align: left;
        }
        .compare-table th {
            background: #f5f5f5;
            font-weight: 600;
        }
        .summary-box {
            background: linear-gradient(135deg, #9C27B0 0%, #BA68C8 100%);
            border-radius: 16px;
            padding: 32px;
            color: white;
            margin: 32px 0;
        }
        .summary-box h3 { font-size: 1.2rem; margin-bottom: 20px; }
        .summary-box ul { list-style: none; }
        .summary-box li {
            padding: 8px 0;
            padding-left: 24px;
            position: relative;
        }
        .summary-box li::before {
            content: "→";
            position: absolute;
            left: 0;
            color: #f3e5f5;
        }
        .nav-links {
            display: flex;
            justify-content: space-between;
            margin-top: 60px;
            padding-top: 24px;
            border-top: 1px solid #e0e0e0;
        }
        .nav-links a { color: #9C27B0; text-decoration: none; font-weight: 500; }
        .nav-links a:hover { text-decoration: underline; }
    </style>
</head>
<body>
    <div class="container">
        <nav class="breadcrumb">
            <a href="../../">AI 공부</a>
            <span>/</span>
            <a href="../">Section 4</a>
            <span>/</span>
            Topic 04
        </nav>

        <header class="header">
            <div class="topic-number">TOPIC 04</div>
            <h1>활성화 함수는 왜 필요해?</h1>
            <p>Q. 선형 변환만 하면 안 돼?</p>
        </header>

        <div class="conversation">
            <h2>Part 1. 활성화 함수가 뭐야?</h2>

            <div class="chat">
                <div class="q">활성화 함수(Activation Function)가 뭔데?</div>
                <div class="a">
                    <p><span class="key-point">가중합 결과를 변형하는 함수</span>야.</p>
                    <p>퍼셉트론에서 마지막 단계에 거치는 필터라고 생각하면 돼.</p>
                </div>
            </div>

            <div class="code-block">퍼셉트론의 흐름:

1. 입력받기: x₁, x₂, x₃
2. 가중합 계산: z = w₁x₁ + w₂x₂ + w₃x₃ + b
3. 활성화 함수 적용: output = f(z)  ← 이 부분!

예:
z = 2.5
f(z) = sigmoid(2.5) = 0.924  (0~1 사이로 변환)

z = -1.3
f(z) = ReLU(-1.3) = 0  (음수를 0으로)</div>

            <div class="chat">
                <div class="q">그냥 z를 출력하면 안 돼?</div>
                <div class="a">
                    <p><span class="warning">그러면 신경망이 무용지물이 돼.</span></p>
                    <p>왜인지 설명할게.</p>
                </div>
            </div>
        </div>

        <div class="conversation">
            <h2>Part 2. 선형 변환만 하면 안 되는 이유</h2>

            <div class="chat">
                <div class="q">활성화 함수 없이 그냥 가중합만 하면 뭐가 문제야?</div>
                <div class="a">
                    <p><span class="highlight">층을 아무리 쌓아도 1층짜리랑 똑같아져.</span></p>
                </div>
            </div>

            <div class="code-block">수학적 증명:

활성화 함수 없이 2층 신경망:

층1: y = W₁x + b₁
층2: z = W₂y + b₂

z를 x로 풀면:
z = W₂(W₁x + b₁) + b₂
z = W₂W₁x + W₂b₁ + b₂
z = (W₂W₁)x + (W₂b₁ + b₂)
z = Wx + b  ← 1층짜리랑 똑같음!


3층이든 100층이든 마찬가지:
결국 y = Wx + b 형태로 합쳐짐

→ 층을 쌓는 의미가 없어짐!</div>

            <div class="chat">
                <div class="a">
                    <p>이게 바로 <span class="warning">"선형성의 저주"</span>야.</p>
                    <p>직선의 조합은 결국 직선이거든.</p>
                </div>
            </div>

            <div class="code-block">비유:

활성화 함수 없음 = 포토샵에서 밝기만 조절
- 밝기 +10, 밝기 +20, 밝기 +30
- 결국 밝기 +60이랑 똑같음

활성화 함수 있음 = 필터, 왜곡, 블러 등 다양한 효과
- 각 층마다 다른 변형 가능
- 조합하면 복잡한 효과 생성</div>
        </div>

        <div class="conversation">
            <h2>Part 3. 대표적인 활성화 함수들</h2>

            <table class="compare-table">
                <tr>
                    <th>함수</th>
                    <th>수식</th>
                    <th>특징</th>
                </tr>
                <tr>
                    <td><strong>Sigmoid</strong></td>
                    <td>σ(x) = 1 / (1 + e⁻ˣ)</td>
                    <td>0~1 사이 출력, 확률로 해석 가능</td>
                </tr>
                <tr>
                    <td><strong>Tanh</strong></td>
                    <td>tanh(x)</td>
                    <td>-1~1 사이 출력, 중심이 0</td>
                </tr>
                <tr>
                    <td><strong>ReLU</strong></td>
                    <td>max(0, x)</td>
                    <td>음수는 0, 양수는 그대로</td>
                </tr>
                <tr>
                    <td><strong>Leaky ReLU</strong></td>
                    <td>max(0.01x, x)</td>
                    <td>음수도 약간 살림</td>
                </tr>
                <tr>
                    <td><strong>Softmax</strong></td>
                    <td>eˣⁱ / Σeˣʲ</td>
                    <td>확률 분포로 변환 (분류용)</td>
                </tr>
            </table>

            <div class="code-block">각 함수의 그래프:

Sigmoid:
   1 |         ╱─────
     |       ╱
   0 |─────╱
     |_______________
    -5   0   5


ReLU:
     |        ╱
     |      ╱
   0 |────╱
     |_______________
    -5   0   5


Tanh:
   1 |       ╱───
     |     ╱
   0 |───╱
     | ╱
  -1 |───
     |_______________
    -5   0   5</div>
        </div>

        <div class="conversation">
            <h2>Part 4. 활성화 함수의 역할</h2>

            <div class="chat">
                <div class="q">그래서 정확히 무슨 역할을 하는 거야?</div>
                <div class="a">
                    <p>크게 3가지야.</p>
                </div>
            </div>

            <div class="code-block">역할 1: 비선형성 도입

선형만:
- 직선 경계만 그을 수 있음
- XOR 같은 문제 풀 수 없음

비선형:
- 곡선 경계 가능
- 복잡한 패턴 학습 가능


역할 2: 출력 범위 제한

Sigmoid:
- 아무리 큰 입력이 와도 0~1로 제한
- 확률처럼 해석 가능
- 출력이 폭주하지 않음

ReLU:
- 음수를 제거
- 희소성(sparsity) 유도
- 계산이 빠름


역할 3: 기울기 조절

학습할 때 역전파로 기울기를 계산하는데,
활성화 함수가 기울기의 흐름을 조절함

좋은 활성화 함수:
- 기울기가 0이 되지 않음 (학습 계속 가능)
- 계산이 빠름
- 수렴이 빠름</div>
        </div>

        <div class="conversation">
            <h2>Part 5. 왜 ReLU를 제일 많이 써?</h2>

            <div class="chat">
                <div class="q">요즘은 ReLU를 많이 쓴다던데?</div>
                <div class="a">
                    <p>맞아. <span class="highlight">ReLU가 현재 가장 인기 있어.</span></p>
                </div>
            </div>

            <div class="code-block">ReLU의 장점:

1. 계산이 엄청 빠름
   sigmoid: 1 / (1 + e^(-x))  ← 지수 계산 필요
   ReLU: max(0, x)  ← 비교만 하면 됨

2. 기울기 소실 문제 완화
   sigmoid: 입력이 크면 기울기가 0에 가까워짐
   ReLU: 양수 영역에서는 기울기가 항상 1

3. 희소 활성화
   음수를 0으로 만들어서 일부 뉴런만 활성화
   → 효율적인 표현 학습

4. 생물학적 타당성
   실제 뉴런도 임계값 이상에서만 발화


단점:

1. Dying ReLU 문제
   음수 영역에서 기울기가 0
   한번 죽으면 다시 살아나지 못함

해결: Leaky ReLU, PReLU 등 변형 사용</div>

            <div class="chat">
                <div class="a">
                    <p><span class="key-point">간단하고 빠르고 효과적</span>이라서 널리 쓰여.</p>
                </div>
            </div>
        </div>

        <div class="conversation">
            <h2>Part 6. 실제 코드</h2>

            <div class="code-block">import numpy as np

# Sigmoid
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# Tanh
def tanh(x):
    return np.tanh(x)

# ReLU
def relu(x):
    return np.maximum(0, x)

# Leaky ReLU
def leaky_relu(x, alpha=0.01):
    return np.maximum(alpha * x, x)

# Softmax
def softmax(x):
    exp_x = np.exp(x - np.max(x))  # 수치 안정성
    return exp_x / np.sum(exp_x)


# 테스트
x = np.array([-2, -1, 0, 1, 2])

print("Sigmoid:", sigmoid(x))
# [0.119 0.269 0.5 0.731 0.881]

print("ReLU:", relu(x))
# [0 0 0 1 2]

print("Leaky ReLU:", leaky_relu(x))
# [-0.02 -0.01  0.  1.  2.]</div>
        </div>

        <div class="summary-box">
            <h3>핵심 정리</h3>
            <ul>
                <li><strong>비선형성 필수</strong> - 활성화 함수 없으면 층을 쌓아도 1층과 동일</li>
                <li><strong>복잡한 패턴 학습</strong> - 비선형 변환으로 곡선 경계 그리기 가능</li>
                <li><strong>출력 범위 제한</strong> - 값이 폭주하지 않도록 조절</li>
                <li><strong>ReLU가 인기</strong> - 빠르고 효과적, 기울기 소실 완화</li>
                <li><strong>용도별 선택</strong> - 은닉층은 ReLU, 출력층은 Sigmoid/Softmax</li>
            </ul>
        </div>

        <div class="nav-links">
            <a href="../topic03/">← 이전: 가중치와 편향은 뭐야?</a>
            <a href="../topic05/">다음: ReLU가 뭐야? →</a>
        </div>
    </div>
</body>
</html>
