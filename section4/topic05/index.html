<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ReLU가 뭐야? - Section 4</title>
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            font-family: 'Pretendard', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
            background: #fafafa;
            color: #333;
            line-height: 1.9;
        }
        .container { max-width: 800px; margin: 0 auto; padding: 60px 20px; }
        .breadcrumb { margin-bottom: 24px; font-size: 0.9rem; }
        .breadcrumb a { color: #666; text-decoration: none; }
        .breadcrumb a:hover { text-decoration: underline; }
        .breadcrumb span { color: #999; margin: 0 8px; }
        .header { margin-bottom: 48px; }
        .topic-number {
            display: inline-block;
            background: #9C27B0;
            color: white;
            padding: 6px 16px;
            border-radius: 20px;
            font-size: 0.85rem;
            font-weight: 600;
            margin-bottom: 16px;
        }
        .header h1 { font-size: 2rem; font-weight: 700; color: #111; margin-bottom: 12px; }
        .header p { color: #666; font-size: 1.1rem; }
        .conversation {
            background: white;
            border-radius: 16px;
            padding: 32px;
            margin: 32px 0;
            border: 1px solid #e0e0e0;
        }
        .conversation h2 {
            font-size: 1.3rem;
            color: #9C27B0;
            margin-bottom: 24px;
            padding-bottom: 12px;
            border-bottom: 2px solid #f3e5f5;
        }
        .chat { margin-bottom: 20px; }
        .q {
            background: #fff3e0;
            border-left: 4px solid #ff9800;
            padding: 16px 20px;
            margin: 16px 0;
            border-radius: 0 12px 12px 0;
            font-weight: 500;
        }
        .q::before { content: "Q. "; color: #e65100; font-weight: 700; }
        .a { padding: 8px 0 8px 20px; border-left: 4px solid #e0e0e0; margin: 12px 0; }
        .a p { margin-bottom: 12px; }
        .a p:last-child { margin-bottom: 0; }
        .highlight {
            background: #e3f2fd;
            padding: 2px 8px;
            border-radius: 4px;
            font-weight: 600;
            color: #1565c0;
        }
        .key-point {
            background: #f3e5f5;
            padding: 2px 8px;
            border-radius: 4px;
            font-weight: 600;
            color: #6a1b9a;
        }
        .warning {
            background: #ffcdd2;
            padding: 2px 8px;
            border-radius: 4px;
            font-weight: 600;
            color: #c62828;
        }
        .code-block {
            background: #263238;
            border-radius: 8px;
            padding: 16px 20px;
            margin: 16px 0;
            font-family: 'Consolas', monospace;
            font-size: 0.9rem;
            color: #eceff1;
            overflow-x: auto;
            line-height: 1.6;
            white-space: pre-wrap;
        }
        .summary-box {
            background: linear-gradient(135deg, #9C27B0 0%, #BA68C8 100%);
            border-radius: 16px;
            padding: 32px;
            color: white;
            margin: 32px 0;
        }
        .summary-box h3 { font-size: 1.2rem; margin-bottom: 20px; }
        .summary-box ul { list-style: none; }
        .summary-box li {
            padding: 8px 0;
            padding-left: 24px;
            position: relative;
        }
        .summary-box li::before {
            content: "→";
            position: absolute;
            left: 0;
            color: #f3e5f5;
        }
        .nav-links {
            display: flex;
            justify-content: space-between;
            margin-top: 60px;
            padding-top: 24px;
            border-top: 1px solid #e0e0e0;
        }
        .nav-links a { color: #9C27B0; text-decoration: none; font-weight: 500; }
        .nav-links a:hover { text-decoration: underline; }
    </style>
</head>
<body>
    <div class="container">
        <nav class="breadcrumb">
            <a href="../../">AI 공부</a>
            <span>/</span>
            <a href="../">Section 4</a>
            <span>/</span>
            Topic 05
        </nav>

        <header class="header">
            <div class="topic-number">TOPIC 05</div>
            <h1>ReLU가 뭐야?</h1>
            <p>Q. 음수를 0으로 만들면 뭐가 좋은데?</p>
        </header>

        <div class="conversation">
            <h2>Part 1. ReLU는 뭔데?</h2>

            <div class="chat">
                <div class="q">ReLU(렐루)가 정확히 뭐야?</div>
                <div class="a">
                    <p><span class="key-point">Rectified Linear Unit</span>의 약자야.</p>
                    <p>한국말로 하면 "정류된 선형 유닛"인데, 그냥 ReLU라고 불러.</p>
                </div>
            </div>

            <div class="code-block">ReLU 수식:

f(x) = max(0, x)

= {  x   if x > 0
  {  0   if x ≤ 0


예시:
f(5) = 5
f(2.3) = 2.3
f(0) = 0
f(-1) = 0
f(-100) = 0


파이썬 코드:
def relu(x):
    return max(0, x)

# 또는
def relu(x):
    if x > 0:
        return x
    else:
        return 0</div>

            <div class="chat">
                <div class="a">
                    <p>이해됐어?</p>
                    <p><span class="highlight">양수는 그대로, 음수는 0</span></p>
                    <p>그게 다야. 정말 단순하지?</p>
                </div>
            </div>
        </div>

        <div class="conversation">
            <h2>Part 2. 왜 이렇게 단순한 게 좋은 거야?</h2>

            <div class="chat">
                <div class="q">너무 단순한데 이게 왜 좋아?</div>
                <div class="a">
                    <p><span class="key-point">단순해서 좋은 거야!</span></p>
                </div>
            </div>

            <div class="code-block">장점 1: 계산 속도

Sigmoid: f(x) = 1 / (1 + e^(-x))
- 지수 계산 필요
- 나눗셈 필요
- 느림

ReLU: f(x) = max(0, x)
- 비교 한 번
- 엄청 빠름

→ 딥러닝에서는 이 계산을 수백만 번 하므로
  속도 차이가 어마어마함


장점 2: 기울기 소실 문제 완화

Sigmoid:
  입력이 -5 미만이나 5 초과면 기울기가 거의 0
  → 학습이 안 됨 (기울기 소실)

ReLU:
  양수 영역에서는 기울기가 항상 1
  → 학습이 잘 됨


장점 3: 희소 활성화 (Sparse Activation)

Sigmoid:
  모든 뉴런이 0이 아닌 값 출력
  → 모든 뉴런이 계속 작동

ReLU:
  음수 입력받은 뉴런은 0 출력
  → 일부 뉴런만 활성화
  → 효율적!</div>

            <div class="chat">
                <div class="a">
                    <p>생각해봐.</p>
                    <p>뇌도 모든 뉴런이 한꺼번에 발화하지 않잖아?</p>
                    <p>필요한 부분만 활성화되는 게 <strong>더 효율적</strong>이야.</p>
                </div>
            </div>
        </div>

        <div class="conversation">
            <h2>Part 3. 음수를 0으로 만들면 뭐가 좋은데?</h2>

            <div class="chat">
                <div class="q">음수 정보를 버리면 손해 아니야?</div>
                <div class="a">
                    <p>좋은 질문이야. <span class="highlight">버리는 게 아니라 "억제"하는 거</span>야.</p>
                </div>
            </div>

            <div class="code-block">비유: 신호등

Sigmoid (부드러운 신호):
  빨간불(음수): 천천히 가세요 (0.1 속도)
  초록불(양수): 빠르게 가세요 (0.9 속도)
  → 항상 움직임, 애매함

ReLU (명확한 신호):
  빨간불(음수): 정지! (0 속도)
  초록불(양수): 가세요! (원래 속도)
  → 명확한 의사결정


실제 효과:

1. 노이즈 제거
   약한 신호(음수)는 무시하고
   강한 신호(양수)만 전달
   → 중요한 특징만 남음

2. 희소 표현 (Sparse Representation)
   적은 수의 뉴런으로 효과적 표현
   → 과적합 방지
   → 계산 효율 증가

3. 선형성과 비선형성의 조화
   양수 영역: 선형 (학습 쉬움)
   전체: 비선형 (표현력 높음)</div>
        </div>

        <div class="conversation">
            <h2>Part 4. ReLU의 문제점</h2>

            <div class="chat">
                <div class="q">완벽해 보이는데 단점은 없어?</div>
                <div class="a">
                    <p><span class="warning">큰 문제가 하나 있어.</span></p>
                    <p><strong>Dying ReLU 문제</strong>야.</p>
                </div>
            </div>

            <div class="code-block">Dying ReLU 문제:

상황:
1. 어떤 뉴런이 음수 입력을 받음
2. ReLU(음수) = 0 출력
3. 기울기도 0
4. 가중치가 업데이트되지 않음
5. 계속 음수 입력만 받음
6. 영원히 0만 출력 (뉴런 사망)

예시:
입력: -5
출력: 0
기울기: 0
업데이트: 없음
→ 다음에도 음수 받으면 또 0
→ 계속 반복... 영원히 죽어있음


통계:
- 실제로 학습 후 뉴런의 20~40%가 죽어있을 수 있음
- 네트워크 용량의 상당 부분이 낭비됨</div>

            <div class="chat">
                <div class="q">해결 방법은?</div>
                <div class="a">
                    <p>ReLU의 변형들이 있어.</p>
                </div>
            </div>
        </div>

        <div class="conversation">
            <h2>Part 5. ReLU 변형들</h2>

            <div class="code-block">1. Leaky ReLU (새는 ReLU)

f(x) = {  x        if x > 0
       {  0.01x    if x ≤ 0

음수도 아주 작은 값으로 살림
→ Dying ReLU 문제 완화


2. Parametric ReLU (PReLU)

f(x) = {  x     if x > 0
       {  αx    if x ≤ 0

α도 학습 가능한 파라미터
→ 네트워크가 최적의 α를 찾음


3. ELU (Exponential Linear Unit)

f(x) = {  x              if x > 0
       {  α(e^x - 1)     if x ≤ 0

음수 영역도 부드럽게 처리
→ 평균이 0에 가까워짐


4. GELU (Gaussian Error Linear Unit)

f(x) = x · Φ(x)
(Φ는 표준 정규분포의 누적분포함수)

최신 트랜스포머에서 사용
→ GPT, BERT 등


5. Swish / SiLU

f(x) = x · sigmoid(x)

Google이 발견
→ 일부 경우 ReLU보다 성능 좋음</div>

            <div class="code-block">Python 코드:

import numpy as np

def relu(x):
    return np.maximum(0, x)

def leaky_relu(x, alpha=0.01):
    return np.where(x > 0, x, alpha * x)

def elu(x, alpha=1.0):
    return np.where(x > 0, x, alpha * (np.exp(x) - 1))

def swish(x):
    return x * (1 / (1 + np.exp(-x)))


# 테스트
x = np.array([-2, -1, 0, 1, 2])

print("ReLU:", relu(x))
# [0 0 0 1 2]

print("Leaky ReLU:", leaky_relu(x))
# [-0.02 -0.01  0.  1.  2.]

print("ELU:", elu(x))
# [-0.865 -0.632  0.  1.  2.]</div>
        </div>

        <div class="conversation">
            <h2>Part 6. 실전에서는?</h2>

            <div class="chat">
                <div class="q">그래서 뭘 써야 해?</div>
                <div class="a">
                    <p>상황에 따라 다르지만 가이드라인을 줄게.</p>
                </div>
            </div>

            <div class="code-block">추천:

일반적인 경우:
→ ReLU (기본 선택)

Dying ReLU 문제가 심한 경우:
→ Leaky ReLU 또는 PReLU

이미지 관련 (CNN):
→ ReLU 또는 Leaky ReLU

자연어 처리 (Transformer):
→ GELU 또는 Swish

작은 네트워크:
→ ReLU (간단하고 빠름)

큰 네트워크:
→ 실험해보고 선택 (GELU, Swish 등)


실전 꿀팁:
1. 일단 ReLU로 시작
2. 문제 있으면 Leaky ReLU 시도
3. 성능 더 필요하면 GELU/Swish 실험
4. 과적합 문제 있으면 Dropout과 함께 사용</div>
        </div>

        <div class="summary-box">
            <h3>핵심 정리</h3>
            <ul>
                <li><strong>ReLU = max(0, x)</strong> - 양수는 그대로, 음수는 0</li>
                <li><strong>빠르고 효과적</strong> - 계산 간단, 기울기 소실 완화</li>
                <li><strong>희소 활성화</strong> - 일부 뉴런만 활성화되어 효율적</li>
                <li><strong>Dying ReLU 문제</strong> - 뉴런이 영구히 죽을 수 있음</li>
                <li><strong>변형 활용</strong> - Leaky ReLU, GELU 등으로 문제 해결</li>
            </ul>
        </div>

        <div class="nav-links">
            <a href="../topic04/">← 이전: 활성화 함수는 왜 필요해?</a>
            <a href="../topic06/">다음: 층을 왜 여러 개 쌓아? →</a>
        </div>
    </div>
</body>
</html>
