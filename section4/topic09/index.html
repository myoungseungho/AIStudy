<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>기울기 소실 문제는 뭐야? - Section 4</title>
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            font-family: 'Pretendard', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
            background: #fafafa;
            color: #333;
            line-height: 1.9;
        }
        .container { max-width: 800px; margin: 0 auto; padding: 60px 20px; }
        .breadcrumb { margin-bottom: 24px; font-size: 0.9rem; }
        .breadcrumb a { color: #666; text-decoration: none; }
        .breadcrumb a:hover { text-decoration: underline; }
        .breadcrumb span { color: #999; margin: 0 8px; }
        .header { margin-bottom: 48px; }
        .topic-number {
            display: inline-block;
            background: #9C27B0;
            color: white;
            padding: 6px 16px;
            border-radius: 20px;
            font-size: 0.85rem;
            font-weight: 600;
            margin-bottom: 16px;
        }
        .header h1 { font-size: 2rem; font-weight: 700; color: #111; margin-bottom: 12px; }
        .header p { color: #666; font-size: 1.1rem; }
        .conversation {
            background: white;
            border-radius: 16px;
            padding: 32px;
            margin: 32px 0;
            border: 1px solid #e0e0e0;
        }
        .conversation h2 {
            font-size: 1.3rem;
            color: #9C27B0;
            margin-bottom: 24px;
            padding-bottom: 12px;
            border-bottom: 2px solid #f3e5f5;
        }
        .chat { margin-bottom: 20px; }
        .q {
            background: #fff3e0;
            border-left: 4px solid #ff9800;
            padding: 16px 20px;
            margin: 16px 0;
            border-radius: 0 12px 12px 0;
            font-weight: 500;
        }
        .q::before { content: "Q. "; color: #e65100; font-weight: 700; }
        .a { padding: 8px 0 8px 20px; border-left: 4px solid #e0e0e0; margin: 12px 0; }
        .a p { margin-bottom: 12px; }
        .a p:last-child { margin-bottom: 0; }
        .highlight {
            background: #e3f2fd;
            padding: 2px 8px;
            border-radius: 4px;
            font-weight: 600;
            color: #1565c0;
        }
        .key-point {
            background: #f3e5f5;
            padding: 2px 8px;
            border-radius: 4px;
            font-weight: 600;
            color: #6a1b9a;
        }
        .warning {
            background: #ffcdd2;
            padding: 2px 8px;
            border-radius: 4px;
            font-weight: 600;
            color: #c62828;
        }
        .code-block {
            background: #263238;
            border-radius: 8px;
            padding: 16px 20px;
            margin: 16px 0;
            font-family: 'Consolas', monospace;
            font-size: 0.9rem;
            color: #eceff1;
            overflow-x: auto;
            line-height: 1.6;
            white-space: pre-wrap;
        }
        .summary-box {
            background: linear-gradient(135deg, #9C27B0 0%, #BA68C8 100%);
            border-radius: 16px;
            padding: 32px;
            color: white;
            margin: 32px 0;
        }
        .summary-box h3 { font-size: 1.2rem; margin-bottom: 20px; }
        .summary-box ul { list-style: none; }
        .summary-box li {
            padding: 8px 0;
            padding-left: 24px;
            position: relative;
        }
        .summary-box li::before {
            content: "→";
            position: absolute;
            left: 0;
            color: #f3e5f5;
        }
        .nav-links {
            display: flex;
            justify-content: space-between;
            margin-top: 60px;
            padding-top: 24px;
            border-top: 1px solid #e0e0e0;
        }
        .nav-links a { color: #9C27B0; text-decoration: none; font-weight: 500; }
        .nav-links a:hover { text-decoration: underline; }
    </style>
</head>
<body>
    <div class="container">
        <nav class="breadcrumb">
            <a href="../../">AI 공부</a>
            <span>/</span>
            <a href="../">Section 4</a>
            <span>/</span>
            Topic 09
        </nav>

        <header class="header">
            <div class="topic-number">TOPIC 09</div>
            <h1>기울기 소실 문제는 뭐야?</h1>
            <p>Q. 층이 깊으면 왜 학습이 안 돼?</p>
        </header>

        <div class="conversation">
            <h2>Part 1. 기울기 소실 문제란?</h2>

            <div class="chat">
                <div class="q">기울기 소실(Vanishing Gradient)이 뭐야?</div>
                <div class="a">
                    <p><span class="warning">층이 깊어지면 앞쪽 층의 기울기가 0에 가까워지는 현상</span>이야.</p>
                    <p>기울기가 0이면? <strong>학습이 안 돼.</strong></p>
                </div>
            </div>

            <div class="code-block">문제 상황:

10층짜리 신경망:
출력층: 기울기 = 0.5
9층: 기울기 = 0.5 × 0.5 = 0.25
8층: 기울기 = 0.25 × 0.5 = 0.125
7층: 기울기 = 0.125 × 0.5 = 0.0625
6층: 기울기 = 0.03125
5층: 기울기 = 0.015625
4층: 기울기 = 0.0078
3층: 기울기 = 0.0039
2층: 기울기 = 0.00195
1층: 기울기 = 0.00097  ← 거의 0!

→ 입력 가까운 층은 거의 학습 안 됨
→ "소실(Vanishing)"</div>

            <div class="chat">
                <div class="a">
                    <p>이해됐어?</p>
                    <p><span class="highlight">연쇄 법칙 때문에 기울기가 계속 곱해지면서 점점 작아짐</span></p>
                </div>
            </div>
        </div>

        <div class="conversation">
            <h2>Part 2. 왜 발생해?</h2>

            <div class="chat">
                <div class="q">왜 기울기가 사라지는 거야?</div>
                <div class="a">
                    <p>주범은 <span class="warning">Sigmoid 활성화 함수</span>야.</p>
                </div>
            </div>

            <div class="code-block">Sigmoid의 문제:

σ(x) = 1 / (1 + e^(-x))

Sigmoid의 기울기:
- 최대값: 0.25 (x=0일 때)
- x가 크거나 작으면 기울기가 0에 가까움

역전파 시:
기울기 = ∂L/∂w = ∂L/∂y × σ'(x₁) × σ'(x₂) × ... × σ'(xₙ)

각 층마다 0.25 이하를 곱함:
0.25 × 0.25 × 0.25 × ... = 매우 작은 수

예:
10층: 0.25^10 = 0.00000095
→ 사실상 0!


비유:
전화기 게임 (Telephone Game):
- 10명이 메시지를 전달
- 각자 25%만 정확하게 전달
- 마지막 사람은 거의 못 알아듣겠지?

역전파도 마찬가지:
- 10층을 거슬러 올라감
- 각 층에서 기울기가 25%씩 감소
- 첫 번째 층은 거의 신호를 못 받음</div>
        </div>

        <div class="conversation">
            <h2>Part 3. 결과는?</h2>

            <div class="chat">
                <div class="q">기울기 소실되면 정확히 뭐가 문제야?</div>
                <div class="a">
                    <p>3가지 큰 문제가 생겨.</p>
                </div>
            </div>

            <div class="code-block">문제 1: 앞쪽 층이 학습 안 됨

출력 가까운 층:
- 기울기 충분함
- 잘 학습됨
- 가중치가 계속 업데이트

입력 가까운 층:
- 기울기 거의 0
- 학습 안 됨
- 가중치가 거의 안 바뀜

→ 깊은 신경망의 장점이 사라짐!


문제 2: 학습 속도 매우 느림

epoch 1: loss = 2.5
epoch 100: loss = 2.49
epoch 1000: loss = 2.48
epoch 10000: loss = 2.47

→ 거의 개선 안 됨


문제 3: 복잡한 특징 학습 불가

앞쪽 층:
- 간단한 특징 학습 못 함
- 초기 랜덤 가중치 그대로

뒤쪽 층:
- 엉터리 입력받음
- 제대로 된 학습 불가능

→ 전체 네트워크 무용지물</div>
        </div>

        <div class="conversation">
            <h2>Part 4. 해결 방법</h2>

            <div class="chat">
                <div class="q">어떻게 해결해?</div>
                <div class="a">
                    <p>여러 방법이 있어.</p>
                </div>
            </div>

            <div class="code-block">해결책 1: ReLU 사용

Sigmoid: σ'(x) ≤ 0.25
ReLU: f'(x) = 1 (x > 0일 때)

→ 기울기가 1이므로 소실 안 됨!

효과:
10층 Sigmoid: 0.25^10 = 0.00000095
10층 ReLU: 1^10 = 1

→ ReLU가 딥러닝 혁명을 일으킨 이유!


해결책 2: 가중치 초기화 개선

Xavier 초기화:
w ~ N(0, 1/n)
(n은 입력 개수)

He 초기화:
w ~ N(0, 2/n)
(ReLU용)

→ 적절한 범위로 초기화하면 기울기 소실 완화


해결책 3: Batch Normalization

각 층의 출력을 정규화:
y = (x - μ) / σ

→ 값의 범위를 적절하게 유지
→ 기울기 흐름 개선


해결책 4: Residual Connection (ResNet)

y = F(x) + x

→ 기울기가 직접 흐를 수 있는 지름길 제공
→ 100층 이상도 학습 가능!


해결책 5: LSTM (RNN용)

Gating 메커니즘:
- 기울기 흐름 제어
- 장기 의존성 학습 가능</code>
        </div>

        <div class="conversation">
            <h2>Part 5. 반대 문제: 기울기 폭발</h2>

            <div class="chat">
                <div class="q">기울기가 너무 커지는 문제는 없어?</div>
                <div class="a">
                    <p>있어! <span class="warning">기울기 폭발(Exploding Gradient)</span> 문제야.</p>
                </div>
            </div>

            <div class="code-block">기울기 폭발:

가중치가 1보다 크면:
1.5 × 1.5 × 1.5 × ... = 매우 큰 수

10층: 1.5^10 = 57.7
20층: 1.5^20 = 3325
50층: 1.5^50 = 6억

→ 가중치가 inf가 됨
→ NaN 에러 발생


해결책:

1. Gradient Clipping:
   if gradient > threshold:
       gradient = threshold

2. 적절한 학습률:
   너무 큰 학습률 피하기

3. 정규화:
   L1, L2 regularization

4. Batch Normalization:
   값의 범위 제한</div>
        </div>

        <div class="conversation">
            <h2>Part 6. 역사적 의미</h2>

            <div class="code-block">딥러닝 역사:

1980년대:
- 역전파 발명
- 하지만 층을 깊게 쌓으면 학습 안 됨
- 기울기 소실 문제 발견
→ 실망

1990~2000년대:
- "신경망은 3층 이상 못 쌓는다"
- SVM, Random Forest 등 다른 방법 유행
→ AI 겨울

2006년:
- Hinton의 사전 학습 기법
- 기울기 소실 일부 해결
→ 희망

2012년:
- AlexNet이 ReLU 사용
- 기울기 소실 문제 대부분 해결
- 8층 네트워크 학습 성공
→ 딥러닝 혁명!

2015년:
- ResNet이 152층 성공
- Skip connection으로 완전 해결
→ 현재까지 이어짐


결론:
기울기 소실 문제 해결 = 딥러닝의 핵심 돌파구!</div>
        </div>

        <div class="summary-box">
            <h3>핵심 정리</h3>
            <ul>
                <li><strong>기울기 소실</strong> - 층이 깊으면 앞쪽 층의 기울기가 0에 가까워짐</li>
                <li><strong>원인은 Sigmoid</strong> - 최대 기울기가 0.25로 제한됨</li>
                <li><strong>ReLU로 해결</strong> - 기울기가 1이므로 소실 안 됨</li>
                <li><strong>추가 해결책</strong> - Batch Norm, ResNet, 가중치 초기화</li>
                <li><strong>역사적 중요성</strong> - 이 문제 해결이 딥러닝 혁명의 핵심</li>
            </ul>
        </div>

        <div class="nav-links">
            <a href="../topic08/">← 이전: 역전파가 뭐야?</a>
            <a href="../topic10/">다음: 배치 정규화가 뭐야? →</a>
        </div>
    </div>
</body>
</html>
