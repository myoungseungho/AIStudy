<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>단어 임베딩이 뭐야? - Section 6</title>
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            font-family: 'Pretendard', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
            background: #fafafa;
            color: #333;
            line-height: 1.9;
        }
        .container { max-width: 800px; margin: 0 auto; padding: 60px 20px; }
        .breadcrumb { margin-bottom: 24px; font-size: 0.9rem; }
        .breadcrumb a { color: #666; text-decoration: none; }
        .breadcrumb a:hover { text-decoration: underline; }
        .breadcrumb span { color: #999; margin: 0 8px; }
        .header { margin-bottom: 48px; }
        .topic-number {
            display: inline-block;
            background: #E91E63;
            color: white;
            padding: 6px 16px;
            border-radius: 20px;
            font-size: 0.85rem;
            font-weight: 600;
            margin-bottom: 16px;
        }
        .header h1 { font-size: 2rem; font-weight: 700; color: #111; margin-bottom: 12px; }
        .header p { color: #666; font-size: 1.1rem; }
        .conversation {
            background: white;
            border-radius: 16px;
            padding: 32px;
            margin: 32px 0;
            border: 1px solid #e0e0e0;
        }
        .conversation h2 {
            font-size: 1.3rem;
            color: #E91E63;
            margin-bottom: 24px;
            padding-bottom: 12px;
            border-bottom: 2px solid #fce4ec;
        }
        .chat { margin-bottom: 20px; }
        .q {
            background: #fff3e0;
            border-left: 4px solid #ff9800;
            padding: 16px 20px;
            margin: 16px 0;
            border-radius: 0 12px 12px 0;
            font-weight: 500;
        }
        .q::before { content: "Q. "; color: #e65100; font-weight: 700; }
        .a { padding: 8px 0 8px 20px; border-left: 4px solid #e0e0e0; margin: 12px 0; }
        .a p { margin-bottom: 12px; }
        .a p:last-child { margin-bottom: 0; }
        .highlight {
            background: #e3f2fd;
            padding: 2px 8px;
            border-radius: 4px;
            font-weight: 600;
            color: #1565c0;
        }
        .key-point {
            background: #fce4ec;
            padding: 2px 8px;
            border-radius: 4px;
            font-weight: 600;
            color: #c2185b;
        }
        .warning {
            background: #ffcdd2;
            padding: 2px 8px;
            border-radius: 4px;
            font-weight: 600;
            color: #c62828;
        }
        .code-block {
            background: #263238;
            border-radius: 8px;
            padding: 16px 20px;
            margin: 16px 0;
            font-family: 'Consolas', monospace;
            font-size: 0.9rem;
            color: #eceff1;
            overflow-x: auto;
            line-height: 1.6;
            white-space: pre-wrap;
        }
        .compare-table {
            width: 100%;
            border-collapse: collapse;
            margin: 16px 0;
            font-size: 0.9rem;
        }
        .compare-table th, .compare-table td {
            padding: 12px;
            border: 1px solid #e0e0e0;
            text-align: left;
        }
        .compare-table th { background: #f5f5f5; font-weight: 600; }
        .summary-box {
            background: linear-gradient(135deg, #E91E63 0%, #EC407A 100%);
            border-radius: 16px;
            padding: 32px;
            color: white;
            margin: 32px 0;
        }
        .summary-box h3 { font-size: 1.2rem; margin-bottom: 20px; }
        .summary-box ul { list-style: none; }
        .summary-box li {
            padding: 8px 0;
            padding-left: 24px;
            position: relative;
        }
        .summary-box li::before {
            content: "→";
            position: absolute;
            left: 0;
            color: #fce4ec;
        }
        .nav-links {
            display: flex;
            justify-content: space-between;
            margin-top: 60px;
            padding-top: 24px;
            border-top: 1px solid #e0e0e0;
        }
        .nav-links a { color: #E91E63; text-decoration: none; font-weight: 500; }
        .nav-links a:hover { text-decoration: underline; }
    </style>
</head>
<body>
    <div class="container">
        <nav class="breadcrumb">
            <a href="../../">AI 공부</a>
            <span>/</span>
            <a href="../">Section 6</a>
            <span>/</span>
            Topic 04
        </nav>

        <header class="header">
            <div class="topic-number">TOPIC 04</div>
            <h1>단어 임베딩이 뭐야?</h1>
            <p>Q. 비슷한 단어는 가까이 있다고?</p>
        </header>

        <div class="conversation">
            <h2>Part 1. 임베딩의 핵심 아이디어</h2>

            <div class="chat">
                <div class="q">단어 임베딩(Word Embedding)이 뭐야?</div>
                <div class="a">
                    <p><span class="key-point">단어를 의미가 담긴 숫자 벡터로 변환하는 거야.</span></p>
                    <p>원-핫과 다르게, <span class="highlight">비슷한 의미의 단어는 비슷한 벡터로 표현</span>돼.</p>
                </div>
            </div>

            <div class="code-block">비유: 좌표평면에 배치하기

2D 공간에 동물들을 배치한다면:

    크기
     ↑
     |        코끼리(9, 7)
     |
     |  사자(7, 6)
     |
     |    강아지(3, 4)  고양이(4, 3)
     |
     |  햄스터(1, 1)
     |____________________________→ 귀여움

→ 비슷한 동물은 가까이 위치
→ "강아지"와 "고양이"는 서로 가까움
→ "햄스터"와 "코끼리"는 멀리 떨어짐
</div>

            <div class="chat">
                <div class="q">원-핫하고 뭐가 다른 건데?</div>
            </div>

            <table class="compare-table">
                <tr>
                    <th>구분</th>
                    <th>원-핫 인코딩</th>
                    <th>임베딩</th>
                </tr>
                <tr>
                    <td><strong>벡터 크기</strong></td>
                    <td>어휘 크기 (10만 차원)</td>
                    <td>고정 크기 (100~1000 차원)</td>
                </tr>
                <tr>
                    <td><strong>벡터 형태</strong></td>
                    <td>희소 (대부분 0)</td>
                    <td>밀집 (모두 실수)</td>
                </tr>
                <tr>
                    <td><strong>유사도</strong></td>
                    <td>표현 불가 (모두 수직)</td>
                    <td>표현 가능</td>
                </tr>
                <tr>
                    <td><strong>의미</strong></td>
                    <td>없음 (단순 번호)</td>
                    <td>있음 (학습으로 획득)</td>
                </tr>
            </table>
        </div>

        <div class="conversation">
            <h2>Part 2. 임베딩 공간의 마법</h2>

            <div class="chat">
                <div class="q">의미를 담는다는 게 구체적으로 뭐야?</div>
                <div class="a">
                    <p><span class="highlight">벡터 연산으로 의미 관계를 표현할 수 있어.</span></p>
                </div>
            </div>

            <div class="code-block">유명한 예시: Word2Vec

"왕" - "남자" + "여자" ≈ "여왕"

실제 벡터로:
king = [0.5, 0.8, 0.1, 0.3, ...]
man  = [0.3, 0.1, 0.9, 0.2, ...]
woman = [0.3, 0.1, 0.2, 0.8, ...]

king - man + woman = [0.5, 0.8, ...]
                     → queen과 가장 가까운 벡터!


또 다른 예시:
"서울" - "한국" + "일본" ≈ "도쿄"
"크다" - "큰" + "작은" ≈ "작다"
"좋은" - "좋게" + "나쁘게" ≈ "나쁜"

→ 의미의 연산이 가능해짐!
</div>

            <div class="chat">
                <div class="q">어떻게 이게 가능한 거야?</div>
                <div class="a">
                    <p><span class="key-point">AI가 수백만 문장을 읽으면서 패턴을 학습하기 때문이야.</span></p>
                </div>
            </div>

            <div class="code-block">학습 과정:

AI가 이런 문장들을 읽음:
"왕이 성에 산다"
"여왕이 성에 산다"
"남자가 일한다"
"여자가 일한다"

패턴 발견:
- "왕"과 "여왕"은 비슷한 문맥 → 벡터를 가까이 배치
- "남자"와 "여자"도 비슷한 문맥 → 벡터를 가까이 배치
- "왕 vs 여왕"의 차이 ≈ "남자 vs 여자"의 차이
  → 벡터 공간에서 평행한 관계로 학습

결과:
왕 → 여왕의 방향 = 남자 → 여자의 방향
→ 벡터 연산으로 추론 가능!
</div>
        </div>

        <div class="conversation">
            <h2>Part 3. 임베딩은 어떻게 만들어?</h2>

            <div class="chat">
                <div class="q">그 벡터를 누가 만드는 건데?</div>
                <div class="a">
                    <p><span class="highlight">신경망이 자동으로 학습해서 만들어.</span></p>
                    <p>사람이 수동으로 만드는 게 아니야.</p>
                </div>
            </div>

            <div class="code-block">임베딩 학습 과정:

1단계: 랜덤 초기화
"강아지" → [0.01, 0.52, 0.83, ...]  (무작위)
"고양이" → [0.73, 0.21, 0.09, ...]  (무작위)
→ 초기에는 의미 없음


2단계: 문장 읽으면서 조정
"나는 강아지를 키운다"
"나는 고양이를 키운다"
→ "강아지"와 "고양이"가 비슷한 위치에 나옴
→ 두 벡터를 조금씩 가까워지도록 조정


3단계: 수백만 번 반복
수천만 개 문장을 읽으면서 계속 조정
→ 최종적으로 의미를 담은 벡터 완성

"강아지" → [0.32, 0.81, 0.43, ...]
"고양이" → [0.35, 0.78, 0.46, ...]
→ 이제 벡터가 매우 비슷함 (유사도 0.92)
</div>

            <div class="chat">
                <div class="q">차원은 몇 개가 좋아?</div>
            </div>

            <div class="code-block">임베딩 차원 선택:

50차원:
- 빠름, 메모리 적게 씀
- 의미 표현력 낮음
- 소규모 데이터에 적합

100~300차원:
- Word2Vec, GloVe 기본값
- 의미 표현력과 속도 균형
- 일반적으로 가장 많이 사용

768차원:
- BERT 기본값
- 의미 표현력 매우 높음
- 큰 모델, 느림

1536차원:
- GPT-3 임베딩
- 최고 수준의 표현력
- 매우 큰 모델 필요

→ 보통 100~300차원이면 충분
</div>
        </div>

        <div class="conversation">
            <h2>Part 4. 유사도 계산하기</h2>

            <div class="chat">
                <div class="q">두 단어가 얼마나 비슷한지 어떻게 알아?</div>
                <div class="a">
                    <p><span class="key-point">코사인 유사도</span>를 많이 써.</p>
                </div>
            </div>

            <div class="code-block">코사인 유사도 (Cosine Similarity):

두 벡터 사이의 각도로 유사도 측정

                    벡터A
                   ↗ (작은 각도)
                  ↗
원점 ●━━━━━━━━→ 벡터B
                (매우 비슷함)


원점 ●━━━━━━━━→ 벡터A

      ↓ (90도)
      ↓ 벡터B
      (전혀 다름)


공식:
similarity = (A · B) / (|A| × |B|)

결과:
1.0  = 완전히 같음
0.5  = 중간 정도 비슷
0.0  = 관련 없음
-1.0 = 완전 반대


예시:
"강아지" · "고양이" / (|강아지| × |고양이|) = 0.82
"강아지" · "자동차" / (|강아지| × |자동차|) = 0.12

→ "강아지"는 "고양이"와 더 유사함!
</div>

            <div class="chat">
                <div class="q">실제 코드로는?</div>
            </div>

            <div class="code-block">Python 예시:

import numpy as np

# 단어 벡터
dog = np.array([0.2, 0.8, 0.3, 0.9])
cat = np.array([0.3, 0.7, 0.4, 0.8])
car = np.array([0.9, 0.1, 0.2, 0.3])

# 코사인 유사도 계산
def cosine_similarity(a, b):
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

print(cosine_similarity(dog, cat))  # 0.989 (매우 유사)
print(cosine_similarity(dog, car))  # 0.412 (덜 유사)

→ 숫자로 명확하게 비교 가능!
</div>
        </div>

        <div class="conversation">
            <h2>Part 5. 실전 활용</h2>

            <div class="chat">
                <div class="q">임베딩을 어디에 써?</div>
                <div class="a">
                    <p>거의 모든 NLP 작업에 써!</p>
                </div>
            </div>

            <div class="code-block">활용 사례:

1. 검색 엔진
   쿼리: "강아지 키우기"
   → 임베딩: [0.2, 0.8, 0.3, ...]

   문서1: "개 사육법"
   → 임베딩: [0.21, 0.79, 0.31, ...]
   → 유사도: 0.96 (매우 관련 있음!)

   문서2: "자동차 수리"
   → 임베딩: [0.8, 0.1, 0.9, ...]
   → 유사도: 0.12 (관련 없음)


2. 추천 시스템
   사용자가 좋아한 영화: "어벤져스"
   → 임베딩으로 유사한 영화 찾기
   → "아이언맨", "스파이더맨" 추천


3. 감정 분석
   "이 영화 정말 최고!"
   → 각 단어를 임베딩으로 변환
   → 신경망이 긍정/부정 판단


4. 기계 번역
   "I love you" → 각 단어를 임베딩으로
   → 신경망 처리
   → "사랑해요" 출력
</div>

            <div class="chat">
                <div class="q">어떤 임베딩을 써야 해?</div>
            </div>

            <div class="code-block">유명한 임베딩 모델:

Word2Vec (2013, Google)
- 가볍고 빠름
- 단어 하나당 하나의 벡터
- 문맥 무시

GloVe (2014, Stanford)
- Word2Vec과 비슷
- 통계 기반 학습

FastText (2016, Facebook)
- 서브워드 기반
- 오타, 신조어 처리 좋음

BERT (2018, Google)
- 문맥 고려
- 같은 단어도 문장에 따라 다른 벡터
- 현재 가장 많이 사용

GPT (2018~, OpenAI)
- BERT와 비슷하지만 방향이 다름
- 텍스트 생성에 특화

→ 요즘은 BERT 계열이 대세
</div>
        </div>

        <div class="summary-box">
            <h3>핵심 정리</h3>
            <ul>
                <li><strong>임베딩</strong> - 단어를 의미가 담긴 밀집 벡터로 변환</li>
                <li><strong>핵심 특징</strong> - 비슷한 단어는 벡터 공간에서 가까이 위치</li>
                <li><strong>벡터 연산</strong> - "왕" - "남자" + "여자" ≈ "여왕" 같은 의미 연산 가능</li>
                <li><strong>자동 학습</strong> - AI가 대량의 텍스트를 읽으며 스스로 학습</li>
                <li><strong>유사도 측정</strong> - 코사인 유사도로 단어 간 유사성 수치화</li>
            </ul>
        </div>

        <div class="nav-links">
            <a href="../topic03/">← 이전: 원-핫 인코딩은 뭐야?</a>
            <a href="../topic05/">다음: Word2Vec은 어떻게 학습해? →</a>
        </div>
    </div>
</body>
</html>
