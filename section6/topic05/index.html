<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Word2Vec은 어떻게 학습해? - Section 6</title>
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            font-family: 'Pretendard', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
            background: #fafafa;
            color: #333;
            line-height: 1.9;
        }
        .container { max-width: 800px; margin: 0 auto; padding: 60px 20px; }
        .breadcrumb { margin-bottom: 24px; font-size: 0.9rem; }
        .breadcrumb a { color: #666; text-decoration: none; }
        .breadcrumb a:hover { text-decoration: underline; }
        .breadcrumb span { color: #999; margin: 0 8px; }
        .header { margin-bottom: 48px; }
        .topic-number {
            display: inline-block;
            background: #E91E63;
            color: white;
            padding: 6px 16px;
            border-radius: 20px;
            font-size: 0.85rem;
            font-weight: 600;
            margin-bottom: 16px;
        }
        .header h1 { font-size: 2rem; font-weight: 700; color: #111; margin-bottom: 12px; }
        .header p { color: #666; font-size: 1.1rem; }
        .conversation {
            background: white;
            border-radius: 16px;
            padding: 32px;
            margin: 32px 0;
            border: 1px solid #e0e0e0;
        }
        .conversation h2 {
            font-size: 1.3rem;
            color: #E91E63;
            margin-bottom: 24px;
            padding-bottom: 12px;
            border-bottom: 2px solid #fce4ec;
        }
        .chat { margin-bottom: 20px; }
        .q {
            background: #fff3e0;
            border-left: 4px solid #ff9800;
            padding: 16px 20px;
            margin: 16px 0;
            border-radius: 0 12px 12px 0;
            font-weight: 500;
        }
        .q::before { content: "Q. "; color: #e65100; font-weight: 700; }
        .a { padding: 8px 0 8px 20px; border-left: 4px solid #e0e0e0; margin: 12px 0; }
        .a p { margin-bottom: 12px; }
        .a p:last-child { margin-bottom: 0; }
        .highlight {
            background: #e3f2fd;
            padding: 2px 8px;
            border-radius: 4px;
            font-weight: 600;
            color: #1565c0;
        }
        .key-point {
            background: #fce4ec;
            padding: 2px 8px;
            border-radius: 4px;
            font-weight: 600;
            color: #c2185b;
        }
        .warning {
            background: #ffcdd2;
            padding: 2px 8px;
            border-radius: 4px;
            font-weight: 600;
            color: #c62828;
        }
        .code-block {
            background: #263238;
            border-radius: 8px;
            padding: 16px 20px;
            margin: 16px 0;
            font-family: 'Consolas', monospace;
            font-size: 0.9rem;
            color: #eceff1;
            overflow-x: auto;
            line-height: 1.6;
            white-space: pre-wrap;
        }
        .compare-table {
            width: 100%;
            border-collapse: collapse;
            margin: 16px 0;
            font-size: 0.9rem;
        }
        .compare-table th, .compare-table td {
            padding: 12px;
            border: 1px solid #e0e0e0;
            text-align: left;
        }
        .compare-table th { background: #f5f5f5; font-weight: 600; }
        .summary-box {
            background: linear-gradient(135deg, #E91E63 0%, #EC407A 100%);
            border-radius: 16px;
            padding: 32px;
            color: white;
            margin: 32px 0;
        }
        .summary-box h3 { font-size: 1.2rem; margin-bottom: 20px; }
        .summary-box ul { list-style: none; }
        .summary-box li {
            padding: 8px 0;
            padding-left: 24px;
            position: relative;
        }
        .summary-box li::before {
            content: "→";
            position: absolute;
            left: 0;
            color: #fce4ec;
        }
        .nav-links {
            display: flex;
            justify-content: space-between;
            margin-top: 60px;
            padding-top: 24px;
            border-top: 1px solid #e0e0e0;
        }
        .nav-links a { color: #E91E63; text-decoration: none; font-weight: 500; }
        .nav-links a:hover { text-decoration: underline; }
    </style>
</head>
<body>
    <div class="container">
        <nav class="breadcrumb">
            <a href="../../">AI 공부</a>
            <span>/</span>
            <a href="../">Section 6</a>
            <span>/</span>
            Topic 05
        </nav>

        <header class="header">
            <div class="topic-number">TOPIC 05</div>
            <h1>Word2Vec은 어떻게 학습해?</h1>
            <p>Q. 주변 단어로 의미를 배운다는 게 뭐야?</p>
        </header>

        <div class="conversation">
            <h2>Part 1. Word2Vec의 핵심 아이디어</h2>

            <div class="chat">
                <div class="q">Word2Vec이 뭐야?</div>
                <div class="a">
                    <p><span class="key-point">단어를 벡터로 바꾸는 가장 유명한 방법</span>이야. 2013년 구글이 만들었어.</p>
                    <p>핵심은 <span class="highlight">"비슷한 맥락에 나오는 단어는 비슷한 의미"</span>라는 가정이야.</p>
                </div>
            </div>

            <div class="code-block">비유: 친구 관계로 사람 이해하기

철수와 영희의 친구 관계를 보면:

철수의 친구: 민수, 준호, 영수
영희의 친구: 민지, 수지, 지영

→ 철수는 남자일 가능성 높음 (남자 친구들이 많음)
→ 영희는 여자일 가능성 높음 (여자 친구들이 많음)

단어도 마찬가지:
"강아지" 주변: "귀여운", "키우다", "산책"
"고양이" 주변: "귀여운", "키우다", "야옹"
→ 둘 다 비슷한 단어들과 등장 = 비슷한 의미

"자동차" 주변: "빠른", "운전", "도로"
→ 완전히 다른 단어들 = 다른 의미
</div>

            <div class="chat">
                <div class="q">그걸 어떻게 학습해?</div>
                <div class="a">
                    <p>두 가지 방법이 있어.</p>
                </div>
            </div>
        </div>

        <div class="conversation">
            <h2>Part 2. CBOW - 주변 단어로 중심 단어 맞추기</h2>

            <div class="chat">
                <div class="q">CBOW가 뭐야?</div>
                <div class="a">
                    <p><span class="key-point">Continuous Bag of Words</span>의 약자야.</p>
                    <p><span class="highlight">주변 단어들을 보고 가운데 단어를 맞추는 게임</span>이라고 생각하면 돼.</p>
                </div>
            </div>

            <div class="code-block">CBOW 학습 예시:

문장: "나는 강아지를 키운다"

       [나는] [____] [키운다]
              ↓
         여기가 뭘까?

학습 과정:
입력: "나는", "키운다" (주변 단어)
정답: "강아지를" (중심 단어)

AI가 해야 할 일:
"나는"과 "키운다" 사이에 올 단어는?
→ "강아지를"이 정답!


다른 예시:
       [귀여운] [____] [산책]
                ↓
           "강아지를"

       [빠른] [____] [운전]
              ↓
          "자동차를"

→ 같은 위치에 오는 단어는 비슷한 벡터를 갖게 됨
</div>

            <div class="chat">
                <div class="q">왜 이게 의미를 학습하는 거야?</div>
            </div>

            <div class="code-block">학습 원리:

1. 초기 상태 (랜덤):
   "강아지" → [0.1, 0.5, 0.2]
   "고양이" → [0.8, 0.2, 0.9]
   → 아무 의미 없음


2. 학습 중:
   "귀여운 ____ 키우다" → 정답: "강아지"
   "귀여운 ____ 키우다" → 정답: "고양이"

   → 같은 맥락에서 나오니까
   → 두 벡터를 점점 비슷하게 조정


3. 학습 후:
   "강아지" → [0.3, 0.8, 0.4]
   "고양이" → [0.35, 0.75, 0.45]
   → 벡터가 비슷해짐 = 의미가 비슷함을 학습!


반대로:
"빠른 ____ 운전" → 정답: "자동차"
→ 전혀 다른 맥락
→ "자동차" 벡터는 "강아지"와 멀어짐
</div>
        </div>

        <div class="conversation">
            <h2>Part 3. Skip-gram - 중심 단어로 주변 단어 맞추기</h2>

            <div class="chat">
                <div class="q">Skip-gram은 뭐가 다른데?</div>
                <div class="a">
                    <p>CBOW와 정반대야. <span class="key-point">중심 단어를 보고 주변 단어를 맞추는 거야.</span></p>
                </div>
            </div>

            <div class="code-block">Skip-gram 학습 예시:

문장: "나는 강아지를 키운다"

입력: "강아지를" (중심 단어)
정답: "나는", "키운다" (주변 단어)

학습 문제:
"강아지를" 주변에는 어떤 단어가 올까?
→ "나는"이 올 수 있어
→ "키운다"가 올 수 있어


여러 문장에서 반복:
"강아지를" → 주변: "귀여운", "키우다", "산책", "먹이"
"고양이를" → 주변: "귀여운", "키우다", "야옹", "먹이"

→ 주변 단어가 비슷함
→ "강아지"와 "고양이" 벡터를 비슷하게 만듦
</div>

            <div class="chat">
                <div class="q">CBOW vs Skip-gram, 뭐가 더 좋아?</div>
            </div>

            <table class="compare-table">
                <tr>
                    <th>구분</th>
                    <th>CBOW</th>
                    <th>Skip-gram</th>
                </tr>
                <tr>
                    <td><strong>방향</strong></td>
                    <td>주변 → 중심</td>
                    <td>중심 → 주변</td>
                </tr>
                <tr>
                    <td><strong>속도</strong></td>
                    <td>빠름</td>
                    <td>느림</td>
                </tr>
                <tr>
                    <td><strong>성능</strong></td>
                    <td>자주 나오는 단어에 강함</td>
                    <td>드문 단어도 잘 학습</td>
                </tr>
                <tr>
                    <td><strong>추천</strong></td>
                    <td>큰 데이터셋</td>
                    <td>작은 데이터셋</td>
                </tr>
            </table>

            <div class="chat">
                <div class="a">
                    <p><span class="highlight">보통 Skip-gram이 더 좋은 성능을 보여.</span></p>
                    <p>그래서 실전에서는 Skip-gram을 더 많이 써.</p>
                </div>
            </div>
        </div>

        <div class="conversation">
            <h2>Part 4. 학습 트릭들</h2>

            <div class="chat">
                <div class="q">그냥 이렇게만 학습하면 돼?</div>
                <div class="a">
                    <p><span class="warning">아니야.</span> 학습을 빠르고 효과적으로 만드는 몇 가지 트릭이 있어.</p>
                </div>
            </div>

            <div class="chat">
                <div class="q">트릭 1. Negative Sampling</div>
            </div>

            <div class="code-block">문제:
모든 단어 조합을 학습하면 너무 오래 걸림

예시: 어휘가 10만 개면
→ 매번 10만 개 단어 전부 체크해야 함
→ 계산량 폭발!


해결책: Negative Sampling
정답 1개 + 틀린 답 5개만 학습

입력: "강아지"
정답: "키우다" ✓
오답: "컴퓨터" ✗
오답: "비행기" ✗
오답: "수학" ✗
오답: "건물" ✗

→ 6개만 비교 (10만 개 대신!)
→ 학습 속도 엄청 빨라짐
</div>

            <div class="chat">
                <div class="q">트릭 2. Subsampling</div>
            </div>

            <div class="code-block">문제:
"은", "는", "이", "가" 같은 단어는 너무 자주 나옴
→ 의미는 없는데 학습 방해

해결책: 자주 나오는 단어는 일부러 제거

원본: "나는 학교에 간다"
→ "나" [제거] "학교에 간다"

원본: "강아지는 귀엽다"
→ "강아지" [제거] "귀엽다"

→ 의미 있는 단어에 집중!
</code>

            <div class="chat">
                <div class="q">트릭 3. Window Size</div>
            </div>

            <div class="code-block">주변 단어를 몇 개나 볼까?

Window Size = 2:
"나는 [강아지를] 매우 좋아한다"
     ↑    ↑    ↑
   앞2개  중심  뒤2개

→ "나는", "매우"만 주변으로 봄


Window Size = 5:
"나는 정말 [강아지를] 매우 좋아한다"
  ↑   ↑      ↑      ↑   ↑
        앞2개  중심  뒤2개

→ "나는", "정말", "매우", "좋아한다" 모두 봄


권장:
- Window = 5~10: 일반적
- Window 크면: 넓은 의미 (주제)
- Window 작으면: 좁은 의미 (문법)
</div>
        </div>

        <div class="conversation">
            <h2>Part 5. 실전 사용법</h2>

            <div class="chat">
                <div class="q">Word2Vec을 직접 학습해야 해?</div>
                <div class="a">
                    <p><span class="highlight">아니! 대부분 pre-trained 모델을 써.</span></p>
                </div>
            </div>

            <div class="code-block">Pre-trained Word2Vec 사용:

Python에서:
```
from gensim.models import KeyedVectors

# 구글이 학습한 모델 로드 (100억 단어로 학습됨!)
model = KeyedVectors.load_word2vec_format(
    'GoogleNews-vectors-negative300.bin',
    binary=True
)

# 유사한 단어 찾기
print(model.most_similar('dog'))
# [('puppy', 0.86), ('cat', 0.76), ('dogs', 0.73), ...]

# 단어 연산
result = model.most_similar(
    positive=['king', 'woman'],
    negative=['man']
)
print(result[0])
# ('queen', 0.71)
```

→ 바로 사용 가능!
→ 직접 학습할 필요 없음
</div>

            <div class="chat">
                <div class="q">한국어는?</div>
            </div>

            <div class="code-block">한국어 Word2Vec:

여러 pre-trained 모델 존재:
1. 박규병의 한국어 Word2Vec
2. FastText 한국어 모델
3. KoNLPy Word2Vec

사용 예시:
```
# 한국어 모델 로드
ko_model = load_model('ko.bin')

# 유사 단어
print(ko_model.most_similar('강아지'))
# [('개', 0.89), ('고양이', 0.76), ('애완동물', 0.71), ...]

# 단어 연산
result = ko_model.most_similar(
    positive=['왕', '여자'],
    negative=['남자']
)
print(result[0])
# ('여왕', 0.73)
```

→ 한국어도 잘 작동!
</div>

            <div class="chat">
                <div class="q">Word2Vec의 한계는?</div>
            </div>

            <div class="code-block">한계:

1. 문맥 무시
   "사과를 먹다" (과일)
   "사과를 하다" (apology)
   → 둘 다 같은 벡터 사용 (구분 못 함)


2. OOV (Out of Vocabulary)
   학습 안 된 단어는 처리 불가
   "ㅋㅋㅋ", "ㅇㅈ" 같은 신조어 → 에러


3. 정적 임베딩
   한 번 학습하면 고정됨
   새로운 의미 추가 어려움


→ 이 문제들을 해결한 게 BERT, GPT!
→ 하지만 Word2Vec은 여전히 유용함 (가볍고 빠름)
</div>
        </div>

        <div class="summary-box">
            <h3>핵심 정리</h3>
            <ul>
                <li><strong>Word2Vec</strong> - 주변 단어를 보고 의미를 학습하는 임베딩 방법</li>
                <li><strong>두 가지 방법</strong> - CBOW(주변→중심)와 Skip-gram(중심→주변)</li>
                <li><strong>핵심 아이디어</strong> - 비슷한 맥락에 나오는 단어는 비슷한 의미</li>
                <li><strong>학습 트릭</strong> - Negative Sampling, Subsampling으로 효율화</li>
                <li><strong>실전</strong> - Pre-trained 모델을 사용, 직접 학습 불필요</li>
            </ul>
        </div>

        <div class="nav-links">
            <a href="../topic04/">← 이전: 단어 임베딩이 뭐야?</a>
            <a href="../topic06/">다음: 시퀀스 데이터는 뭐야? →</a>
        </div>
    </div>
</body>
</html>
