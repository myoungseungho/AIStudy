<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>정책이 뭐야? - Section 7</title>
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            font-family: 'Pretendard', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
            background: #fafafa;
            color: #333;
            line-height: 1.9;
        }
        .container { max-width: 800px; margin: 0 auto; padding: 60px 20px; }
        .breadcrumb { margin-bottom: 24px; font-size: 0.9rem; }
        .breadcrumb a { color: #666; text-decoration: none; }
        .breadcrumb a:hover { text-decoration: underline; }
        .breadcrumb span { color: #999; margin: 0 8px; }
        .header { margin-bottom: 48px; }
        .topic-number {
            display: inline-block;
            background: #FF5722;
            color: white;
            padding: 6px 16px;
            border-radius: 20px;
            font-size: 0.85rem;
            font-weight: 600;
            margin-bottom: 16px;
        }
        .header h1 { font-size: 2rem; font-weight: 700; color: #111; margin-bottom: 12px; }
        .header p { color: #666; font-size: 1.1rem; }
        .conversation {
            background: white;
            border-radius: 16px;
            padding: 32px;
            margin: 32px 0;
            border: 1px solid #e0e0e0;
        }
        .conversation h2 {
            font-size: 1.3rem;
            color: #FF5722;
            margin-bottom: 24px;
            padding-bottom: 12px;
            border-bottom: 2px solid #ffebe9;
        }
        .chat { margin-bottom: 20px; }
        .q {
            background: #fff3e0;
            border-left: 4px solid #ff9800;
            padding: 16px 20px;
            margin: 16px 0;
            border-radius: 0 12px 12px 0;
            font-weight: 500;
        }
        .q::before { content: "Q. "; color: #e65100; font-weight: 700; }
        .a { padding: 8px 0 8px 20px; border-left: 4px solid #e0e0e0; margin: 12px 0; }
        .a p { margin-bottom: 12px; }
        .a p:last-child { margin-bottom: 0; }
        .highlight {
            background: #e3f2fd;
            padding: 2px 8px;
            border-radius: 4px;
            font-weight: 600;
            color: #1565c0;
        }
        .key-point {
            background: #ffccbc;
            padding: 2px 8px;
            border-radius: 4px;
            font-weight: 600;
            color: #d84315;
        }
        .warning {
            background: #ffcdd2;
            padding: 2px 8px;
            border-radius: 4px;
            font-weight: 600;
            color: #c62828;
        }
        .code-block {
            background: #263238;
            border-radius: 8px;
            padding: 16px 20px;
            margin: 16px 0;
            font-family: 'Consolas', monospace;
            font-size: 0.9rem;
            color: #eceff1;
            overflow-x: auto;
            line-height: 1.6;
            white-space: pre-wrap;
        }
        .compare-table {
            width: 100%;
            border-collapse: collapse;
            margin: 16px 0;
            font-size: 0.9rem;
        }
        .compare-table th, .compare-table td {
            padding: 12px;
            border: 1px solid #e0e0e0;
            text-align: left;
        }
        .compare-table th { background: #f5f5f5; font-weight: 600; }
        .summary-box {
            background: linear-gradient(135deg, #FF5722 0%, #FF7043 100%);
            border-radius: 16px;
            padding: 32px;
            color: white;
            margin: 32px 0;
        }
        .summary-box h3 { font-size: 1.2rem; margin-bottom: 20px; }
        .summary-box ul { list-style: none; }
        .summary-box li { padding: 8px 0; padding-left: 24px; position: relative; }
        .summary-box li::before { content: "→"; position: absolute; left: 0; color: #ffccbc; }
        .nav-links {
            display: flex;
            justify-content: space-between;
            margin-top: 60px;
            padding-top: 24px;
            border-top: 1px solid #e0e0e0;
        }
        .nav-links a { color: #FF5722; text-decoration: none; font-weight: 500; }
        .nav-links a:hover { text-decoration: underline; }
    </style>
</head>
<body>
    <div class="container">
        <nav class="breadcrumb">
            <a href="../../">AI 공부</a>
            <span>/</span>
            <a href="../">Section 7</a>
            <span>/</span>
            Topic 06
        </nav>

        <header class="header">
            <div class="topic-number">TOPIC 06</div>
            <h1>정책이 뭐야?</h1>
            <p>Q. 상황마다 어떤 행동을 할지 정하는 규칙?</p>
        </header>

        <div class="conversation">
            <h2>Part 1. 정책(Policy)이란?</h2>

            <div class="chat">
                <div class="q">정책이 정확히 뭐야?</div>
                <div class="a">
                    <p><span class="key-point">정책은 "상태를 보고 행동을 선택하는 전략"</span>이야.</p>
                    <p>간단히 말하면 <strong>"상황에서 뭘 할지 정하는 규칙"</strong>이지.</p>
                </div>
            </div>

            <div class="code-block">비유: 운전 규칙

정책 = 운전 매뉴얼

상태: 빨간 신호등
행동: 멈춘다

상태: 앞차가 브레이크
행동: 나도 브레이크

상태: 고속도로 진입
행동: 속도를 높인다

→ 정책은 "모든 상황에서 뭘 할지" 정해놓은 것</div>

            <div class="chat">
                <div class="q">수식으로는 어떻게 표현해?</div>
                <div class="a">
                    <p>정책은 보통 π(파이)로 표현해.</p>
                </div>
            </div>

            <div class="code-block">정책 표기법:

π(s) = a

"상태 s에서 행동 a를 선택"


또는 확률적 정책:

π(a|s) = 확률

"상태 s에서 행동 a를 할 확률"


예시:
π(점프 | 적이 앞에 있음) = 0.9  (90% 확률)
π(공격 | 적이 앞에 있음) = 0.1  (10% 확률)</div>
        </div>

        <div class="conversation">
            <h2>Part 2. 결정적 정책 vs 확률적 정책</h2>

            <div class="chat">
                <div class="q">정책도 종류가 있어?</div>
                <div class="a">
                    <p>두 가지 주요 타입이 있어.</p>
                </div>
            </div>

            <table class="compare-table">
                <tr>
                    <th>타입</th>
                    <th>설명</th>
                    <th>예시</th>
                </tr>
                <tr>
                    <td><strong>결정적 정책</strong><br/>(Deterministic)</td>
                    <td>같은 상태 → 항상 같은 행동</td>
                    <td>빨간불 → 무조건 멈춤</td>
                </tr>
                <tr>
                    <td><strong>확률적 정책</strong><br/>(Stochastic)</td>
                    <td>같은 상태 → 확률적으로 행동 선택</td>
                    <td>적 발견 → 90% 공격, 10% 회피</td>
                </tr>
            </table>

            <div class="code-block">결정적 정책:

def policy(state):
    if state == "적이 앞에":
        return "공격"
    elif state == "체력 부족":
        return "도망"
    elif state == "아이템 발견":
        return "획득"


확률적 정책:

def policy(state):
    if state == "적이 앞에":
        return random_choice([
            ("공격", 0.7),
            ("방어", 0.2),
            ("도망", 0.1)
        ])

→ 같은 상황에서도 다양한 행동</div>

            <div class="chat">
                <div class="q">왜 확률적 정책을 써?</div>
                <div class="a">
                    <p><span class="highlight">탐험 때문</span>이야.</p>
                    <p>확률적으로 행동을 선택하면 자연스럽게 탐험이 돼.</p>
                    <p>그리고 상대가 예측하기 어려워져서 <strong>게임 AI에서 유리</strong>해.</p>
                </div>
            </div>
        </div>

        <div class="conversation">
            <h2>Part 3. 정책 학습</h2>

            <div class="chat">
                <div class="q">정책은 어떻게 배우는 거야?</div>
                <div class="a">
                    <p>강화학습의 목표가 바로 <span class="key-point">"좋은 정책을 찾는 것"</span>이야.</p>
                </div>
            </div>

            <div class="code-block">정책 학습 과정:

1. 초기 정책 (랜덤)
   π₀: 모든 상태에서 랜덤하게 행동

2. 경험 수집
   정책을 따라 게임 플레이
   → 상태, 행동, 보상 기록

3. 정책 평가
   "이 정책은 얼마나 좋을까?"
   → 받은 보상들로 평가

4. 정책 개선
   "어떻게 하면 더 나아질까?"
   → 보상이 높았던 행동을 더 자주 선택

5. 반복
   π₁ → π₂ → π₃ → ... → π* (최적 정책)</div>

            <div class="code-block">예시: 팩맨 정책 학습

초기 정책:
상태: 유령이 위에 → 행동: 랜덤 (위/아래/좌/우)
→ 자주 죽음

개선된 정책 (100회 후):
상태: 유령이 위에 → 행동: 아래 (70%), 좌우 (각 15%)
→ 생존율 증가

최적 정책 (10000회 후):
상태: 유령이 위에 → 행동: 최적 도망 경로
→ 높은 점수</div>
        </div>

        <div class="conversation">
            <h2>Part 4. 최적 정책</h2>

            <div class="chat">
                <div class="q">최적 정책은 뭐야?</div>
                <div class="a">
                    <p><span class="highlight">가능한 모든 정책 중 가장 높은 누적 보상을 주는 정책</span>이야.</p>
                </div>
            </div>

            <div class="code-block">최적 정책 π*:

모든 상태에서 최선의 행동을 선택

π*(s) = argmax Q(s, a)
        a

→ 각 상태에서 가치(Q값)가 가장 높은 행동 선택


예시: 체스
π*: 모든 판 상황에서 이길 확률이 가장 높은 수

예시: 자율주행
π*: 안전하고 빠르게 목적지에 도달하는 운전</div>

            <div class="chat">
                <div class="q">최적 정책은 항상 찾을 수 있어?</div>
                <div class="a">
                    <p><span class="warning">이론적으로는 존재하지만, 실제로 찾기는 매우 어려워.</span></p>
                    <p>상태 공간이 너무 크거나, 환경이 복잡하면 거의 불가능해.</p>
                    <p>그래서 <strong>"충분히 좋은" 정책</strong>을 찾는 게 목표야.</p>
                </div>
            </div>
        </div>

        <div class="conversation">
            <h2>Part 5. 정책의 표현 방법</h2>

            <div class="chat">
                <div class="q">정책을 어떻게 저장하고 표현해?</div>
                <div class="a">
                    <p>여러 가지 방법이 있어.</p>
                </div>
            </div>

            <div class="code-block">1. 테이블 정책 (Tabular Policy)

상태가 적을 때 사용:

정책 테이블:
┌──────────┬────────┐
│ 상태     │ 행동   │
├──────────┼────────┤
│ 적 위    │ 아래   │
│ 적 아래  │ 위     │
│ 적 좌    │ 우     │
│ 적 우    │ 좌     │
└──────────┴────────┘

→ 단순하지만 상태가 많으면 불가능</div>

            <div class="code-block">2. 함수 근사 (Function Approximation)

신경망으로 정책 표현:

입력: 상태 (게임 화면)
    ↓
신경망 (여러 층)
    ↓
출력: 각 행동의 확률
    [위: 0.1, 아래: 0.7, 좌: 0.1, 우: 0.1]

→ 복잡한 상태도 처리 가능
→ 딥러닝 기반 강화학습</div>

            <div class="code-block">3. 규칙 기반 정책

사람이 만든 규칙:

if 체력 < 20:
    return "회복 포션 사용"
elif 적과 거리 < 5:
    return "공격"
elif 아이템 보임:
    return "아이템으로 이동"
else:
    return "탐험"

→ 간단한 환경에 유용
→ 강화학습으로 개선 가능</div>
        </div>

        <div class="conversation">
            <h2>Part 6. 정책 vs 가치 함수</h2>

            <div class="chat">
                <div class="q">정책이랑 가치 함수가 뭐가 달라?</div>
                <div class="a">
                    <p>둘은 다른 관점에서 문제를 접근해.</p>
                </div>
            </div>

            <div class="code-block">차이점:

정책 (Policy):
→ "이 상태에서 뭘 할까?"
→ 직접적인 행동 규칙
→ π(s) = a

가치 함수 (Value Function):
→ "이 상태는 얼마나 좋을까?"
→ 상태/행동의 가치 평가
→ V(s) = 예상 누적 보상


관계:
좋은 정책 → 높은 가치
높은 가치 → 좋은 정책 유도</div>

            <div class="code-block">예시:

상태: 체스 판 상황

정책 접근:
→ "이 말을 여기로 이동한다"
→ 직접 행동 명령

가치 함수 접근:
→ "이 판은 +0.7점 (이길 가능성 높음)"
→ "이 수를 두면 +0.8점이 되네"
→ 가치가 높아지는 수 선택

→ 결과적으로 같은 수를 두지만 접근 방식이 다름</div>

            <div class="chat">
                <div class="a">
                    <p><span class="key-point">정책 기반 vs 가치 기반 방법</span>은 다음 토픽들에서 자세히 다룰게.</p>
                </div>
            </div>
        </div>

        <div class="summary-box">
            <h3>📌 핵심 정리</h3>
            <ul>
                <li><strong>정책</strong> - 상태에서 행동을 선택하는 전략</li>
                <li><strong>결정적 정책</strong> - 항상 같은 행동 선택</li>
                <li><strong>확률적 정책</strong> - 확률적으로 행동 선택 (탐험에 유리)</li>
                <li><strong>최적 정책</strong> - 최대 누적 보상을 주는 정책</li>
                <li><strong>표현 방법</strong> - 테이블, 신경망, 규칙 기반</li>
            </ul>
        </div>

        <div class="nav-links">
            <a href="../topic05/">← 이전: 탐험과 활용이 뭐야?</a>
            <a href="../topic07/">다음: 가치 함수가 뭐야? →</a>
        </div>
    </div>
</body>
</html>
