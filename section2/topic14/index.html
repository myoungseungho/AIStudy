<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>로그는 어디에 쓰여? - Section 2</title>
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            font-family: 'Pretendard', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
            background: #fafafa;
            color: #333;
            line-height: 1.9;
        }
        .container { max-width: 800px; margin: 0 auto; padding: 60px 20px; }
        .breadcrumb { margin-bottom: 24px; font-size: 0.9rem; }
        .breadcrumb a { color: #666; text-decoration: none; }
        .breadcrumb a:hover { text-decoration: underline; }
        .breadcrumb span { color: #999; margin: 0 8px; }
        .header { margin-bottom: 48px; }
        .topic-number {
            display: inline-block;
            background: #FF9800;
            color: white;
            padding: 6px 16px;
            border-radius: 20px;
            font-size: 0.85rem;
            font-weight: 600;
            margin-bottom: 16px;
        }
        .header h1 { font-size: 2rem; font-weight: 700; color: #111; margin-bottom: 12px; }
        .header p { color: #666; font-size: 1.1rem; }
        .conversation {
            background: white;
            border-radius: 16px;
            padding: 32px;
            margin: 32px 0;
            border: 1px solid #e0e0e0;
        }
        .conversation h2 {
            font-size: 1.3rem;
            color: #FF9800;
            margin-bottom: 24px;
            padding-bottom: 12px;
            border-bottom: 2px solid #fff3e0;
        }
        .chat { margin-bottom: 20px; }
        .q {
            background: #fff3e0;
            border-left: 4px solid #ff9800;
            padding: 16px 20px;
            margin: 16px 0;
            border-radius: 0 12px 12px 0;
            font-weight: 500;
        }
        .q::before { content: "Q. "; color: #e65100; font-weight: 700; }
        .a {
            padding: 8px 0 8px 20px;
            border-left: 4px solid #e0e0e0;
            margin: 12px 0;
        }
        .a p { margin-bottom: 12px; }
        .a p:last-child { margin-bottom: 0; }
        .highlight {
            background: #e3f2fd;
            padding: 2px 8px;
            border-radius: 4px;
            font-weight: 600;
            color: #1565c0;
        }
        .key-point {
            background: #fff3e0;
            padding: 2px 8px;
            border-radius: 4px;
            font-weight: 600;
            color: #e65100;
        }
        .warning {
            background: #ffcdd2;
            padding: 2px 8px;
            border-radius: 4px;
            font-weight: 600;
            color: #c62828;
        }
        .code-block {
            background: #263238;
            border-radius: 8px;
            padding: 16px 20px;
            margin: 16px 0;
            font-family: 'Consolas', monospace;
            font-size: 0.9rem;
            color: #eceff1;
            overflow-x: auto;
            line-height: 1.6;
            white-space: pre-wrap;
        }
        .compare-table {
            width: 100%;
            border-collapse: collapse;
            margin: 16px 0;
            font-size: 0.9rem;
        }
        .compare-table th, .compare-table td {
            padding: 12px;
            border: 1px solid #e0e0e0;
            text-align: left;
        }
        .compare-table th {
            background: #f5f5f5;
            font-weight: 600;
        }
        .summary-box {
            background: linear-gradient(135deg, #FF9800 0%, #FFB74D 100%);
            border-radius: 16px;
            padding: 32px;
            color: white;
            margin: 32px 0;
        }
        .summary-box h3 { font-size: 1.2rem; margin-bottom: 20px; }
        .summary-box ul { list-style: none; }
        .summary-box li {
            padding: 8px 0;
            padding-left: 24px;
            position: relative;
        }
        .summary-box li::before {
            content: "→";
            position: absolute;
            left: 0;
            color: #fff3e0;
        }
        .nav-links {
            display: flex;
            justify-content: space-between;
            margin-top: 60px;
            padding-top: 24px;
            border-top: 1px solid #e0e0e0;
        }
        .nav-links a { color: #FF9800; text-decoration: none; font-weight: 500; }
        .nav-links a:hover { text-decoration: underline; }
    </style>
</head>
<body>
    <div class="container">
        <nav class="breadcrumb">
            <a href="../../">AI 공부</a>
            <span>/</span>
            <a href="../">Section 2</a>
            <span>/</span>
            Topic 14
        </nav>

        <header class="header">
            <div class="topic-number">TOPIC 14</div>
            <h1>로그는 어디에 쓰여?</h1>
            <p>Q. 곱셈을 덧셈으로 바꾸면 뭐가 좋은데?</p>
        </header>

        <div class="conversation">
            <h2>Part 1. 로그가 뭐였지?</h2>

            <div class="chat">
                <div class="q">로그(log)가 뭐야?</div>
                <div class="a">
                    <p><span class="key-point">지수의 역함수</span></p>
                </div>
            </div>

            <div class="code-block">지수와 로그:

2³ = 8  ←→  log₂(8) = 3

읽기: "2를 몇 번 곱해야 8이 되나? 3번"


일반 형태:
a^b = c  ←→  log_a(c) = b


자연로그 (ln):
밑이 e (약 2.718...)
log_e(x) = ln(x)

AI에서는 대부분 자연로그 사용!


기본 성질:
log(ab) = log(a) + log(b)  ← 곱셈→덧셈
log(a/b) = log(a) - log(b)  ← 나눗셈→뺄셈
log(a^b) = b×log(a)         ← 지수→곱셈
log(1) = 0
log(e) = 1</div>
        </div>

        <div class="conversation">
            <h2>Part 2. 왜 AI에 로그를 쓰나?</h2>

            <div class="chat">
                <div class="q">AI에서 로그가 왜 필요한 거야?</div>
                <div class="a">
                    <p>여러 이유가 있어:</p>
                </div>
            </div>

            <div class="code-block">이유 1: 큰 수 다루기

확률 곱셈:
P = 0.9 × 0.8 × 0.7 × ... (100개)
  = 0.00000265...  ← 너무 작아짐!

로그 사용:
log P = log(0.9) + log(0.8) + log(0.7) + ...
      = -0.105 + (-0.223) + (-0.357) + ...
      = -6.577

→ 언더플로 방지!


이유 2: 곱셈을 덧셈으로

곱셈: 느림, 오차 누적
덧셈: 빠름, 안정적

log(a×b×c) = log(a) + log(b) + log(c)


이유 3: 스케일 조정

원본: [1, 10, 100, 1000]
로그: [0, 1, 2, 3]

→ 큰 범위를 작게 압축</div>
        </div>

        <div class="conversation">
            <h2>Part 3. Cross-Entropy Loss</h2>

            <div class="chat">
                <div class="q">로그가 손실함수에 왜 들어가?</div>
                <div class="a">
                    <p><span class="highlight">Cross-Entropy Loss의 핵심</span></p>
                </div>
            </div>

            <div class="code-block">Binary Cross-Entropy:

L = -[y×log(p) + (1-y)×log(1-p)]

예시:
정답: y = 1 (고양이)
예측: p = 0.9 (90% 확신)

L = -[1×log(0.9) + 0×log(0.1)]
  = -log(0.9)
  ≈ 0.105


예측이 틀리면:
정답: y = 1
예측: p = 0.1 (10% 확신)

L = -log(0.1)
  ≈ 2.303

→ 손실이 크게 증가!


왜 로그?
1. 확률 0에 가까우면 무한대 패널티
   log(0.001) = -6.9
   log(0.00001) = -11.5

2. 미분이 깔끔
   d(-log(p))/dp = -1/p

3. 정보 이론과 연결
   → 엔트로피 개념</div>
        </div>

        <div class="conversation">
            <h2>Part 4. LogSoftmax</h2>

            <div class="chat">
                <div class="q">LogSoftmax는 왜 쓰는 거야?</div>
                <div class="a">
                    <p><span class="key-point">수치 안정성</span></p>
                </div>
            </div>

            <div class="code-block">문제:

x = [1000, 2000, 3000]

Softmax:
e^1000 = overflow!  ← 컴퓨터가 계산 못함


해결: LogSoftmax

LogSoftmax(x_i) = x_i - log(Σe^(x_j))

트릭:
log(Σe^(x_j)) = max(x) + log(Σe^(x_j - max(x)))

→ 큰 수를 빼서 안정화


실제 계산:
x = [1000, 2000, 3000]
max = 3000

x - max = [-2000, -1000, 0]
e^[-2000, -1000, 0] = [0, 0, 1]  ← 계산 가능!

LogSoftmax = [-2000, -1000, 0]</div>
        </div>

        <div class="conversation">
            <h2>Part 5. 활성화 함수</h2>

            <div class="chat">
                <div class="q">다른 곳에서는 어디에 쓰여?</div>
                <div class="a">
                    <p>여러 곳에 숨어있어:</p>
                </div>
            </div>

            <div class="code-block">Sigmoid:

σ(x) = 1 / (1 + e^(-x))

로그 형태로 표현하면:
log(σ(x)) = -log(1 + e^(-x))

→ 계산 안정화


LogSumExp:

log(e^(x₁) + e^(x₂) + ... + e^(xₙ))

→ Softmax 계산에 사용


정보 이론:

엔트로피: H = -Σp(x)log(p(x))
KL 발산: D_KL = Σp(x)log(p(x)/q(x))

→ 분포 간 차이 측정</div>
        </div>

        <div class="conversation">
            <h2>Part 6. 코드 예시</h2>

            <div class="chat">
                <div class="q">실제로 어떻게 써?</div>
                <div class="a">
                    <p>PyTorch 예시:</p>
                </div>
            </div>

            <div class="code-block">import torch
import torch.nn.functional as F
import numpy as np

# 자연로그
x = torch.tensor([1.0, 2.718, 10.0])
log_x = torch.log(x)
print(log_x)  # [0.0000, 1.0000, 2.3026]

# Cross-Entropy Loss
pred = torch.tensor([0.9])
target = torch.tensor([1.0])

# 직접 계산
loss = -target * torch.log(pred)
print(loss)  # 0.1054

# LogSoftmax
logits = torch.tensor([2.0, 1.0, 0.1])
log_probs = F.log_softmax(logits, dim=0)
print(log_probs)
# tensor([-0.4170, -1.4170, -2.3170])

# 일반 Softmax와 비교
probs = F.softmax(logits, dim=0)
print(torch.log(probs))
# 같은 결과!

# NLLLoss (LogSoftmax와 함께 사용)
criterion = nn.NLLLoss()
log_probs = F.log_softmax(logits.unsqueeze(0), dim=1)
target = torch.tensor([0])
loss = criterion(log_probs, target)
print(loss)  # 0.4170</div>
        </div>

        <div class="summary-box">
            <h3>핵심 정리</h3>
            <ul>
                <li><strong>로그 = 지수의 역함수</strong> - log_a(b) = c ⇔ a^c = b</li>
                <li><strong>곱셈을 덧셈으로</strong> - log(ab) = log(a) + log(b)</li>
                <li><strong>큰 수 안정화</strong> - 언더플로/오버플로 방지</li>
                <li><strong>Cross-Entropy 핵심</strong> - 분류 손실함수에 필수</li>
                <li><strong>LogSoftmax 사용</strong> - 수치 안정성</li>
                <li><strong>정보 이론 연결</strong> - 엔트로피, KL 발산</li>
            </ul>
        </div>

        <div class="nav-links">
            <a href="../topic13/">← 이전: 소프트맥스가 뭐야?</a>
            <a href="../topic15/">다음: 왜 수식이 이렇게 복잡해 보여? →</a>
        </div>
    </div>
</body>
</html>
